imagePullSecrets:
  #- name: my-registry-secret


###############################################################################
modelCache:
  persistence:
    storageClassName: "local-path"
    size: "10Gi"
    accessModes:
      - "ReadWriteOnce"
      #- "ReadWriteMany"
    annotations:
      "helm.sh/resource-policy": "keep"
      "kubernetes.io/pvc-protection": "true"


###############################################################################
modelServers:
  VLLM:
    images:
      default: "vllm/vllm-openai:v0.8.3"
  OLlama:
    images:
      default: "ollama/ollama:latest"


###############################################################################
resourceProfiles:
  cpu:
    imageName: "cpu"
    requests:
      cpu: 2
      # TODO: Consider making this a ratio that is more common on cloud machines
      # such as 1:4 CPU:Mem. NOTE: This might need to be adjusted for local clusters.
      memory: "8Gi"
      # TODO: Consider adding eph storage requests/limits.
      # Perhaps this is just needed for GKE Autopilot which defaults
      # to 1Gi for CPU-only.
      # ephemeral-storage: "2Gi"
    limits:
      cpu: 12
      memory: "24Gi"
  
  nvidia-gpu:
    imageName: "nvidia-gpu"
    runtimeClassName: "nvidia"
    requests:
      nvidia.com/gpu: "1"
      cpu: 2
      memory: "4Gi"
    limits:
      nvidia.com/gpu: "1"
      cpu: 12
      memory: "8Gi"
    # tolerations:
    #   - key: "nvidia.com/gpu"
    #     operator: "Equal"
    #     value: "present"
    #     effect: "NoSchedule"
    # nodeSelector:
    #   nvidia.com/gpu.family: "ampere"
    #   nvidia.com/gpu.memory: "81614088"



###############################################################################
catalog:
 
  gemma3-1b-ollama-cpu:
    enabled: false
    features: [TextGeneration]
    url: "ollama://gemma3:1b"
    engine: OLlama
    resourceProfile: cpu

  gemma3-1b-ollama-gpu:
    enabled: true
    features: [TextGeneration]
    url: "ollama://gemma3:1b"
    engine: OLlama
    resourceProfile: nvidia-gpu

  deepseek-r1-distill-qwen-1.5b-vllm-gpu:
    enabled: false
    features: ["TextGeneration"]
    url: "hf://deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
    engine: VLLM
    env:
      VLLM_USE_V1: "1"
    args:
    - --max-model-len=2048
    - --max-num-batched-token=2048
    - --max-num-seqs=8
    resourceProfile: nvidia-gpu
