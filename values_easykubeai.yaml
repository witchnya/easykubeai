imagePullSecrets:
  #- name: my-registry-secret


###############################################################################
modelCache:
  persistence:
    storageClassName: "local-path"
    size: "10Gi"
    accessModes:
      - "ReadWriteOnce"
      #- "ReadWriteMany"
    annotations:
      "helm.sh/resource-policy": "keep"
      "kubernetes.io/pvc-protection": "true"


###############################################################################
secrets:
  huggingface:
    create: false
    token: ""


###############################################################################
modelServers:
  VLLM:
    images:
      default: "vllm/vllm-openai:v0.8.3"
  OLlama:
    images:
      default: "ollama/ollama:latest"


###############################################################################
resourceProfiles:
  cpu:
    # requests:
    #   cpu: 2
    #   memory: "8Gi"
    # limits:
    #   cpu: 12
    #   memory: "24Gi"
  
  nvidia-gpu:
    # runtimeClassName: "nvidia"
    # requests:
    #   nvidia.com/gpu: "1"
    #   cpu: 1
    #   memory: "6Gi"
    # limits:
    #   nvidia.com/gpu: "1"
    #   cpu: 2
    #   memory: "12Gi"
    # tolerations:
    #   - key: "nvidia.com/gpu"
    #     operator: "Equal"
    #     value: "present"
    #     effect: "NoSchedule"
    # nodeSelector:
    #   nvidia.com/gpu.family: "ampere"
    #   nvidia.com/gpu.memory: "81614088"



###############################################################################
catalog:

  gemma3-1b-ollama-cpu:
    enabled: false

  gemma3-1b-ollama-gpu:
    enabled: false

  deepseek-r1-distill-qwen-1.5b-vllm-gpu:
    enabled: false

  qwen2.5-coder-1.5b-vllm-gpu:
    enabled: false

  qwen3-1.7b-vllm-gpu:
    enabled: true

  qwen3-4b-instruct-2507-vllm-gpu:
    enabled: false

###############################################################################
