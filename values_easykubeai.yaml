
###############################################################################
modelCache:
  mode: "pvc" # "host" or "pvc"
  host:
    hostPath: "/mnt/c/model" #"/mnt/c/some/path/for/model"
  pvc:
    storageClassName: "local-path"
    size: "10Gi"
    annotations:
      "helm.sh/resource-policy": "keep"
      "kubernetes.io/pvc-protection": "true"


###############################################################################
secrets:
  huggingface:
    create: true
    token: ""


###############################################################################
resourceProfiles:
  cpu:
    # requests:
    #   cpu: 2
    #   memory: "8Gi"
    # limits:
    #   cpu: 12
    #   memory: "24Gi"
  
  nvidia-gpu:
    # runtimeClassName: "nvidia"
    # requests:
    #   nvidia.com/gpu: "1"
    #   cpu: 2
    #   memory: "4Gi"
    # limits:
    #   nvidia.com/gpu: "1"
    #   cpu: 4
    #   memory: "8Gi"
    # tolerations:
    #   - key: "nvidia.com/gpu"
    #     operator: "Equal"
    #     value: "present"
    #     effect: "NoSchedule"
    # nodeSelector:
    #   nvidia.com/gpu.family: "ampere"
    #   nvidia.com/gpu.memory: "81614088"



###############################################################################
catalog:

  "gemma3-1b-ollama-cpu":
    enabled: true

  "gemma3-1b-ollama-gpu":
    enabled: false

  "deepseek-r1-distill-qwen-1.5b-vllm-gpu":
    enabled: false

  "qwen2.5-coder-1.5b-vllm-gpu":
    enabled: false

  "qwen3-1.7b-vllm-gpu":
    enabled: false

  "gemma3-1b-it-vllm-gpu":
    enabled: false


###############################################################################
