{{- range $serveName, $model := .Values.catalog }}
  {{- if $model.enabled }}
  {{- if eq $model.engine "VLLM" }}
    {{- $modelName := (trimPrefix "hf://" $model.url) }}
    {{- $rpName := $model.resourceProfile | default "cpu" }}
    {{- $rp := index $.Values.resourceProfiles $rpName }}
    {{- $rcn := get $rp "runtimeClassName" }}
    {{- $mountPath := "/root/.cache/huggingface" }}

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: model-deploy-{{ regexReplaceAll "[^a-zA-Z0-9-]" (lower $serveName) "-" }}
  labels:
    app: "model-{{ $serveName }}"
    model: "{{ regexReplaceAll "[^a-zA-Z0-9-.]" $modelName "-" }}"
  annotations:
    resourceProfile: {{ $rpName }}
spec:
  replicas: {{ $model.replicas | default 1 }}

  selector:
    matchLabels:
      app: "model-{{ $serveName }}"
      model: "{{ regexReplaceAll "[^a-zA-Z0-9-.]" $modelName "-" }}"
  template:
    metadata:
      labels:
        app: "model-{{ $serveName }}"
        model: "{{ regexReplaceAll "[^a-zA-Z0-9-.]" $modelName "-" }}"
    spec:
      {{- with $.Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}

      {{- with $rcn }}
      runtimeClassName: {{ . }}
      {{- end }}

      initContainers:
      - name: vllm-init
        image: {{ $.Values.modelServers.VLLM.images.default }}
        imagePullPolicy: {{ default $.Values.modelServers.OLlama.imagePullPolicy "IfNotPresent" }} 
        env:
        - name: HF_HOME
          value: "/root/.cache/huggingface"
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              key: token
              name: {{ $.Values.secrets.huggingface.name | default "secret-hf" }}
              optional: true
        command:
        - /bin/sh
        - -c
        - |

          echo "# init"
          
          if [ -n "$HF_TOKEN" ]; then
            echo "# huggingface-cli login"
            huggingface-cli login --token "$HF_TOKEN"
          fi
          echo "# huggingface-cli download {{ $modelName }} "
          huggingface-cli download {{ $modelName }} 

          set -x
          nvidia-smi

        resources:
          limits:
            cpu: {{ ($rp).limits.cpu }}
            memory: {{ ($rp).limits.memory }}  

        volumeMounts:
        {{- if eq $.Values.modelCache.mode "pvc" }}
        - name: models-volume
          mountPath: {{ $mountPath }}
        {{- else if eq $.Values.modelCache.mode "host" }}
        - name: host-path
          mountPath: {{ $mountPath }}
        {{- end }}

      containers:
      - name: vllm-server
        image: {{ $.Values.modelServers.VLLM.images.default }}
        imagePullPolicy: {{ default $.Values.modelServers.OLlama.imagePullPolicy "IfNotPresent" }} 
        env:
        - name: HF_HOME
          value: "/root/.cache/huggingface"
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              key: token
              name: kubeai-huggingface
              optional: true
        {{- range $k, $v := $model.env }}
        - name:  {{ $k }}
          value: {{ $v | quote }}
        {{- end }}

        command:
        - python3
        - -m
        - vllm.entrypoints.openai.api_server
        args:
        - --port=80
        - --model={{ $modelName }}
        - --served-model-name={{ regexReplaceAll "[^a-zA-Z0-9-]" $serveName "-" }}
        {{- with $model.args }}
        {{- . | toYaml | nindent 8 }}
        {{- end }}
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /health
            port: http
            scheme: HTTP
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 3
        ports:
        - containerPort: 80
          name: http
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /health
            port: http
            scheme: HTTP
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 2

        resources:
          # $model 의 resourceProfile 에서 가져온 값은 프로파일명 
          limits:
            cpu: {{ ($rp).limits.cpu }}
            memory: {{ ($rp).limits.memory }}  

        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: false
          runAsUser: 0
        startupProbe:
          failureThreshold: 5400
          httpGet:
            path: /health
            port: http
            scheme: HTTP
          periodSeconds: 2
          successThreshold: 1
          timeoutSeconds: 2
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - name: dshm
          mountPath: /dev/shm
        {{- if eq $.Values.modelCache.mode "pvc" }}
        - name: models-volume
          mountPath: {{ $mountPath }}
        {{- else if eq $.Values.modelCache.mode "host" }}
        - name: host-path
          mountPath: {{ $mountPath }}
        {{- end }}

      terminationGracePeriodSeconds: 30

      volumes:
      - name: dshm
        emptyDir:
          medium: Memory
      {{- if eq $.Values.modelCache.mode "pvc" }}
      - name: models-volume
        persistentVolumeClaim:
          claimName: model-pvc-vllm-{{ regexReplaceAll "[^a-zA-Z0-9-]" (lower $modelName) "-" }}
      {{- else if eq $.Values.modelCache.mode "host" }}
      - name: host-path
        hostPath: 
          path: {{ $.Values.modelCache.host.hostPath }}
          type: Directory
      {{- end }}
          
      {{- with $model.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with $model.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with $model.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}

{{- end }}
{{- end }}
{{- end }}