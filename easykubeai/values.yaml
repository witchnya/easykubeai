imagePullSecrets:
#- name: my-registry-secret

###############################################################################

ui:
  enabled: true
  image: "python:3.13-slim-bookworm"
  replicas: 1
  resources: {}
  nodeSelector: {}
  affinity: {}
  tolerations: []
  pvc:
    storageClassName: ""
    size: "5Gi"
    accessModes:
      - "ReadWriteOnce"
      #- "ReadWriteMany"
    annotations:
      "helm.sh/resource-policy": "keep"
      "kubernetes.io/pvc-protection": "true"

ingress:
  enabled: true
  className: ""
  annotations: {}
  tls: []
  hosts:
    - host: easykubeai.local
      paths:
      - path: /
        pathType: Prefix

###############################################################################
modelCache:
  mode: "pvc" # "host" or "pvc"
  host:
    hostPath: "/mnt/c/some/path/for/model" #"/mnt/c/some/path/for/model"
  pvc:
    storageClassName: ""
    size: "10Gi"
    accessModes:
      - "ReadWriteOnce"
      #- "ReadWriteMany"
    annotations:
      "helm.sh/resource-policy": "keep"
      "kubernetes.io/pvc-protection": "true"

service:
  port: 80
  annotations:


###############################################################################
secrets:
  huggingface:
    create: false
    token: ""
    name: "secret-hf"


###############################################################################
modelServers:
  VLLM:
    images:
      default: "vllm/vllm-openai:v0.8.3"
  OLlama:
    images:
      default: "ollama/ollama:latest"


###############################################################################
resourceProfiles:
  cpu:
    requests:
      cpu: 4
      # TODO: Consider making this a ratio that is more common on cloud machines
      # such as 1:4 CPU:Mem. NOTE: This might need to be adjusted for local clusters.
      memory: "8Gi"
      # TODO: Consider adding eph storage requests/limits.
      # Perhaps this is just needed for GKE Autopilot which defaults
      # to 1Gi for CPU-only.
      # ephemeral-storage: "2Gi"
    limits:
      cpu: 12
      memory: "24Gi"
  
  nvidiaGpu:
    runtimeClassName: "nvidia"
    requests:
      nvidia.com/gpu: "1"
      cpu: 4
      memory: "4Gi"
    limits:
      nvidia.com/gpu: "1"
      cpu: 8
      memory: "8Gi"
    # tolerations:
    #   - key: "nvidia.com/gpu"
    #     operator: "Equal"
    #     value: "present"
    #     effect: "NoSchedule"
    # nodeSelector:
    #   nvidia.com/gpu.family: "ampere"
    #   nvidia.com/gpu.memory: "81614088"


###############################################################################
catalog:

  #============================================================================
  # OLLAMA CPU

  # Deepseek cpu #
  "deepseek-r1-1.5b-ollama-cpu":
    enabled: false
    features: [ TextGeneration ]
    url: 'ollama://deepseek-r1:1.5b'
    modelSize: 1.1Gi
    engine: OLlama
    resourceProfile: cpu


  # Gemma cpu #
  "gemma3-1b-ollama-cpu":
    enabled: false
    features: [ TextGeneration ]
    url: "ollama://gemma3:1b"
    modelSize: 815Mi
    engine: OLlama
    resourceProfile: cpu


  # Qwen cpu #
  "qwen2.5-coder-1.5b-ollama-cpu":
    enabled: false
    features: [ "TextGeneration" ]
    url: "ollama://qwen2.5-coder:1.5b"
    modelSize: 986Mi
    engine: OLlama
    resourceProfile: cpu
  "qwen3-1.7b-ollama-cpu":
    enabled: false
    features: [ "TextGeneration" ]
    url: "ollama://qwen3:1.7b"
    modelSize: 1.4Gi
    engine: OLlama
    resourceProfile: cpu


  # Llama cpu #
  llama3.2-1b-ollama-cpu:
    enabled: false
    features: [ "TextGeneration" ]
    url: "ollama://llama3.2:1b"
    modelSize: 1.3Gi
    engine: OLlama
    resourceProfile: cpu


  # embed cpu #
  nomic-embed-text-ollama-cpu:
    enabled: false
    features: [ "TextEmbedding" ]
    url: "ollama://nomic-embed-text"
    modelSize: 274Mi
    engine: OLlama
    resourceProfile: cpu


  #============================================================================
  # OLLAMA GPU

  # Deepseek #
  "deepseek-r1-1.5b-ollama-gpu":
    enabled: false
    features: [ TextGeneration ]
    url: 'ollama://deepseek-r1:1.5b'
    modelSize: 1.1Gi
    engine: OLlama
    resourceProfile: nvidiaGpu
  "deepseek-r1-distill-qwen-7b-ollama-gpu":
    enabled: false
    features: [ TextGeneration ]
    url: 'ollama://deepseek-r1:7b'
    modelSize: 4.7Gi
    engine: OLlama
    resourceProfile: nvidiaGpu


  # Gemma #
  "gemma3-1b-ollama-gpu":
    enabled: false
    features: [ TextGeneration ]
    url: "ollama://gemma3:1b"
    modelSize: 815Mi
    engine: OLlama
    resourceProfile: nvidiaGpu
  "gemma3-4b-ollama-gpu":
    enabled: false
    features: [ TextGeneration ]
    url: 'ollama://gemma3:4b'
    modelSize: 3.3Gi
    engine: OLlama
    resourceProfile: nvidiaGpu


  # Qwen #
  "qwen2.5-coder-1.5b-ollama-gpu":
    enabled: false
    features: [ "TextGeneration" ]
    url: "ollama://qwen2.5-coder:1.5b"
    modelSize: 986Mi
    engine: OLlama
    resourceProfile: nvidiaGpu
  "qwen3-1.7b-ollama-gpu":
    enabled: false
    features: [ "TextGeneration" ]
    url: "ollama://qwen3:1.7b"
    modelSize: 1.4Gi
    engine: OLlama
    resourceProfile: nvidiaGpu
  "qwen3-4b-ollama-gpu":
    enabled: false
    features: [ "TextGeneration" ]
    url: "ollama://qwen3:4b"
    modelSize: 2.5Gi
    engine: OLlama
    resourceProfile: nvidiaGpu


  # Llama #
  "llama3.1-8b-ollama-gpu":
    enabled: false
    features: [ "TextGeneration" ]
    url: "ollama://llama3.1:8b"
    modelSize: 4.9Gi
    engine: OLlama
    resourceProfile: nvidiaGpu
  "llama3.2-1b-ollama-gpu":
    enabled: false
    features: [ "TextGeneration" ]
    url: "ollama://llama3.2:1b"
    modelSize: 1.3Gi
    engine: OLlama
    resourceProfile: nvidiaGpu
  "llama3.2-3b-ollama-gpu":
    enabled: false
    features: [ "TextGeneration" ]
    url: "ollama://llama3.2:3b"
    modelSize: 2.0Gi
    engine: OLlama
    resourceProfile: nvidiaGpu


  #============================================================================
  # VLLM GPU

  # Deepseek vllm #
  "deepseek-r1-distill-qwen-1.5b-vllm-gpu":
    enabled: false
    features: [ "TextGeneration" ]
    url: "hf://deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
    engine: VLLM
    env:
      VLLM_USE_V1: "1"
    args:
      - --max-model-len=2048
      - --max-num-batched-token=2048
      - --max-num-seqs=8
    resourceProfile: nvidiaGpu


  # Qwen vllm #
  "qwen2.5-coder-1.5b-vllm-gpu":
    enabled: false
    features: [ "TextGeneration" ]
    url: "hf://Qwen/Qwen2.5-Coder-1.5B-Instruct"
    engine: VLLM
    env:
      VLLM_ATTENTION_BACKEND: FLASHINFER
    args:
      - --max-model-len=2048
      - --max-num-seqs=16
      - --quantization=fp8
      - --kv-cache-dtype=fp8
    resourceProfile: nvidiaGpu
  "qwen3-1.7b-vllm-gpu":
    enabled: false
    features: [ "TextGeneration" ]
    url: "hf://Qwen/Qwen3-1.7B"
    engine: VLLM
    env:
    args:
      - --max-model-len=2048
      - --max-num-seqs=16
      #- --enable-reasoning
      #- --reasoning-parser=deepseek_r1
    resourceProfile: nvidiaGpu


  # Gemma vllm #
  "gemma3-1b-it-vllm-gpu":
    enabled: false
    features: [ "TextGeneration" ]
    url: "hf://google/gemma-3-1b-it"
    engine: VLLM
    env:
      #VLLM_LOGGING_LEVEL: DEBUG
    args:
      - --max-model-len=2048
      - --max-num-seqs=16
    resourceProfile: nvidiaGpu


###############################################################################
